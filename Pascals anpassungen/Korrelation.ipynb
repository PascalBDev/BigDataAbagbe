{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da4b367",
   "metadata": {},
   "source": [
    "#### Analysieren von Amazon Review Daten.\n",
    "In diesem Skript ein Datensatz bestehend aus Review Daten des Versandhändlers Amazon analysiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d413f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import sys, math\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331207e",
   "metadata": {},
   "source": [
    "bei .master (\"local[*]\") einsetzen, oder ein master + slaves starten (steigert Performance um das 4-fache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46dc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Amazon Reviews\") \\\n",
    "    .getOrCreate()\n",
    "# .master(\"spark://192.168.1.247:7077\") \\\n",
    "\n",
    "# Lade JSON Datei\n",
    "df = spark.read.json(r'/Users/pascal/Desktop/Big Data/All_Beauty.json')\n",
    "\n",
    "# Entferne Spalten\n",
    "df = df.drop('verified', 'reviewTime',\"reviewerID\", \"asin\",\"reviewerName\",\"summary\",\"unixReviewTime\",\"vote\", \"style\", \"image\")\n",
    "\n",
    "# replacing any null values in the 'reviewText' column with an empty space\n",
    "df = df.fillna(\" \", subset=[\"reviewText\"])\n",
    "df = df.withColumn(\"Textlänge_integer\", pyspark.sql.functions.length('reviewText'))\n",
    "\n",
    "# casting the \"Textlänge_integer\" column to a double data type and renaming it to \"Textlänge\"\n",
    "df = df.withColumn(\"Textlänge\", col(\"Textlänge_integer\").cast(\"double\"))\n",
    "df = df.drop(\"reviewText\", \"Textlänge_integer\")\n",
    "\n",
    "# replacing any null values in the \"Textlänge\" column with 0\n",
    "df.fillna(0, subset=[\"Textlänge\"])\n",
    "\n",
    "# Dataframe in RDD konvertieren.\n",
    "partitionen = 10\n",
    "data = df.rdd.repartition(partitionen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d5f82",
   "metadata": {},
   "source": [
    "Korrelation step by step selber berechnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(data, column):\n",
    "    return data.map(lambda x: x[column]).sum() / n\n",
    "\n",
    "def calculate_std(data, column, mean):\n",
    "    return math.sqrt(data.map(lambda x: (x[column] - mean)**2).sum() / n)\n",
    "\n",
    "def calculate_cov(data, column1, column2, mean1, mean2):\n",
    "    return data.map(lambda x: (x[column1] - mean1) * (x[column2] - mean2)).sum() / n\n",
    "\n",
    "start_time = time.time()\n",
    "n = data.count()\n",
    "mean_col1 = calculate_mean(data, 0)\n",
    "mean_col2 = calculate_mean(data, 1)\n",
    "std_col1 = calculate_std(data, 0, mean_col1)\n",
    "std_col2 = calculate_std(data, 1, mean_col2)\n",
    "covXY = calculate_cov(data, 0, 1, mean_col1, mean_col2)\n",
    "correlation = covXY / (std_col1 * std_col2)\n",
    "\n",
    "print(\"Korrelation: \", correlation)\n",
    "print(f\"Execution time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7fccba",
   "metadata": {},
   "source": [
    "# Pyspark.SQL Korrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1351665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korrelation berechnen\n",
    "start_time = time.time()\n",
    "correlation = pyspark.sql.DataFrameStatFunctions(df).corr(\"overall\", \"Textlänge\")\n",
    "print(\"Korrelation: \", correlation)\n",
    "print(f\"Execution time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f1d5e",
   "metadata": {},
   "source": [
    "# Pandas Korrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ac180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame in ein Pandas_Dataframe umwandeln\n",
    "start_time = time.time()\n",
    "data = df.toPandas()\n",
    "print(f\"Execution time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korrelation mit Panda-DataFrame berechnen\n",
    "start_time = time.time()\n",
    "print(data.corr(numeric_only = True))\n",
    "print(f\"Execution time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f4490",
   "metadata": {},
   "source": [
    "# Code um Duplikate zu bilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(r'C:\\Users\\klass\\data\\all_beauty.json')\n",
    "\n",
    "# Inhalt duplizieren\n",
    "df = df.union(df)\n",
    "df = df.coalesce(1)\n",
    "\n",
    "# Inhalt in eine Datei schreiben\n",
    "df.write.json(\"duplicated_large_file.json\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
